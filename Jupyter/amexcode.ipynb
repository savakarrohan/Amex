{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbcc0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from pandas.api.types import is_numeric_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5883575d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Step 1: Loading data...\n"
     ]
    }
   ],
   "source": [
    "# ────────── Step 1: Load CSVs ──────────\n",
    "print(\"🔹 Step 1: Loading data...\")\n",
    "train               = pd.read_csv(\"train_data.csv\")\n",
    "test_local          = pd.read_csv(\"test_data.csv\")\n",
    "submission_template = pd.read_csv(\"submission_template.csv\")\n",
    "offer               = pd.read_csv(\"offer_metadata.csv\")\n",
    "event               = pd.read_csv(\"add_event.csv\")\n",
    "trans               = pd.read_csv(\"add_trans.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b8c5abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Step 1.5: Parsing id1 for exact event time...\n"
     ]
    }
   ],
   "source": [
    "# ────── Step 1.5: Parse id1 into event_seconds ──────\n",
    "print(\"🔹 Step 1.5: Parsing id1 for exact event time...\")\n",
    "for df in [train, test_local]:\n",
    "    parts      = df['id1'].str.split(' ', n=1, expand=True)\n",
    "    time_str   = parts[1]\n",
    "    tp         = pd.to_datetime(time_str, format='%H:%M:%S.%f', errors='coerce')\n",
    "    df['event_seconds'] = (\n",
    "        tp.dt.hour * 3600 +\n",
    "        tp.dt.minute * 60 +\n",
    "        tp.dt.second +\n",
    "        tp.dt.microsecond / 1e6\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44a521e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Step 2: Parsing dates...\n"
     ]
    }
   ],
   "source": [
    "# ────────── Step 2: Parse date fields ──────────\n",
    "print(\"🔹 Step 2: Parsing dates...\")\n",
    "train['id4'] = pd.to_datetime(train['id4'])\n",
    "test_local['id4'] = pd.to_datetime(test_local['id4'])\n",
    "offer['id12'], offer['id13'] = pd.to_datetime(offer['id12']), pd.to_datetime(offer['id13'])\n",
    "event['id4'], event['id7']   = pd.to_datetime(event['id4']), pd.to_datetime(event['id7'])\n",
    "trans['f370']                = pd.to_datetime(trans['f370'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "252a2ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Step 3: Creating event-level aggregates...\n"
     ]
    }
   ],
   "source": [
    "# ────────── Step 3: Build event‐level aggregates ──────────\n",
    "print(\"🔹 Step 3: Creating event-level aggregates...\")\n",
    "event['clicked']     = event['id7'].notna().astype(int)\n",
    "event['click_delay'] = (event['id7'] - event['id4']).dt.total_seconds().clip(lower=0)\n",
    "\n",
    "user_event_agg = (\n",
    "    event.groupby('id2')\n",
    "         .agg(total_impressions=('id4','count'),\n",
    "              unique_offers_seen=('id3','nunique'),\n",
    "              total_clicks=('clicked','sum'),\n",
    "              avg_click_delay=('click_delay','mean'),\n",
    "              median_click_delay=('click_delay','median'),\n",
    "              max_click_delay=('click_delay','max'))\n",
    "         .reset_index()\n",
    ")\n",
    "\n",
    "offer_ctr = (\n",
    "    event.groupby('id3')\n",
    "         .agg(offer_impressions=('id4','count'),\n",
    "              offer_clicks=('clicked','sum'))\n",
    "         .assign(offer_ctr=lambda df: df.offer_clicks / df.offer_impressions)\n",
    "         .reset_index()[['id3','offer_ctr']]\n",
    ")\n",
    "\n",
    "user_offer_seen = (\n",
    "    event.groupby(['id2','id3'])\n",
    "         .size()\n",
    "         .reset_index(name='user_offer_seen_count')\n",
    ")\n",
    "\n",
    "event_ids = event[['id2','id3','id4','id6']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e0746d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Step 4: Creating transaction-level aggregates...\n"
     ]
    }
   ],
   "source": [
    "# ────────── Step 4: Build transaction‐level aggregates ──────────\n",
    "print(\"🔹 Step 4: Creating transaction-level aggregates...\")\n",
    "txn_agg = (\n",
    "    trans.groupby('id2')\n",
    "         .agg(total_spent=('f367','sum'),\n",
    "              avg_spent=('f367','mean'),\n",
    "              txn_count=('f367','count'),\n",
    "              spend_std=('f367','std'),\n",
    "              last_tx=('f370','max'))\n",
    "         .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fb933e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 New: Computing CTR by hour & day_of_week...\n"
     ]
    }
   ],
   "source": [
    "# *** New: Step 4.5 – CTR by hour & day_of_week\n",
    "print(\"🔹 New: Computing CTR by hour & day_of_week...\")\n",
    "hour_ctr = (\n",
    "    event.assign(hour=event['id4'].dt.hour)\n",
    "         .groupby('hour')['clicked']\n",
    "         .mean()\n",
    "         .reset_index(name='hour_ctr')\n",
    ")\n",
    "dow_ctr = (\n",
    "    event.assign(dow=event['id4'].dt.dayofweek)\n",
    "         .groupby('dow')['clicked']\n",
    "         .mean()\n",
    "         .reset_index(name='dow_ctr')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b3fb6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Step 5: Defining enrichment function...\n"
     ]
    }
   ],
   "source": [
    "# ────────── Step 5: Define enrichment function ──────────\n",
    "print(\"🔹 Step 5: Defining enrichment function...\")\n",
    "def enrich(df):\n",
    "    # 5.1) Merge metadata\n",
    "    df = df.merge(offer,           on='id3', how='left')\n",
    "    df = df.merge(user_event_agg,  on='id2', how='left')\n",
    "    df = df.merge(offer_ctr,       on='id3', how='left')\n",
    "    df = df.merge(txn_agg,         on='id2', how='left')\n",
    "    df = df.merge(user_offer_seen, on=['id2','id3'], how='left')\n",
    "    # 5.2) Temporal features\n",
    "    df['days_since_last_tx'] = (df['id4'] - df['last_tx']).dt.days.clip(lower=0)\n",
    "    df['days_until_start']   = (df['id4'] - df['id12']).dt.days.clip(lower=0)\n",
    "    df['days_until_expiry']  = (df['id13'] - df['id4']).dt.days.clip(lower=0)\n",
    "    # 5.3) Discount & text\n",
    "    df['high_discount']      = (df['f376'] > df['f376'].median()).astype(int)\n",
    "    df['offer_body_length']  = df['f378'].astype(str).str.len().fillna(0)\n",
    "    # ── Existing “simple” features ──\n",
    "    df['hour']        = df['id4'].dt.hour\n",
    "    df['day_of_week'] = df['id4'].dt.dayofweek\n",
    "    df['is_weekend']  = (df['day_of_week'] >= 5).astype(int)\n",
    "    df['offer_duration_days']  = (df['id13'] - df['id12']).dt.days\n",
    "    df['redemption_frequency'] = df['f375']\n",
    "    # *** New: Merge CTR by hour & day‐of‐week\n",
    "    df = df.merge(hour_ctr, on='hour', how='left')\n",
    "    df = df.merge(dow_ctr, left_on='day_of_week', right_on='dow', how='left').drop(columns=['dow'])\n",
    "    # *** New: Percent of offer life elapsed\n",
    "    df['pct_offer_elapsed'] = df['days_until_start'] / (df['offer_duration_days'] + 1e-6)\n",
    "    # event_seconds already in df from Step 1.5\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17457dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Step 6: Merging placement ID & enriching train/test...\n"
     ]
    }
   ],
   "source": [
    "# ────────── Step 6: Merge & enrich train/test ──────────\n",
    "print(\"🔹 Step 6: Merging placement ID & enriching train/test...\")\n",
    "train      = train.merge(event_ids,     on=['id2','id3','id4'], how='left')\n",
    "# *** New: Impression‐sequence per user for train\n",
    "train      = train.sort_values(['id2','id4'])\n",
    "train['imp_seq'] = train.groupby('id2').cumcount() + 1\n",
    "train      = enrich(train)\n",
    "\n",
    "test_local = test_local.merge(event_ids, on=['id2','id3','id4'], how='left')\n",
    "# *** New: Impression‐sequence per user for test_local\n",
    "test_local = test_local.sort_values(['id2','id4'])\n",
    "test_local['imp_seq'] = test_local.groupby('id2').cumcount() + 1\n",
    "test_local = enrich(test_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e5eb057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Step 7: Enriching submission test frame...\n"
     ]
    }
   ],
   "source": [
    "# ────────── Step 7: Build & enrich submission frame ──────────\n",
    "print(\"🔹 Step 7: Enriching submission test frame...\")\n",
    "test_full = pd.read_csv(\"test_data.csv\")\n",
    "test_full['id4'] = pd.to_datetime(test_full['id4'])\n",
    "\n",
    "submission_merged = (\n",
    "    submission_template[['id1']]\n",
    "    .merge(test_full,    on='id1', how='left')\n",
    "    .merge(event_ids,    on=['id2','id3','id4'], how='left')\n",
    ")\n",
    "# parse id1 → event_seconds for submission_merged\n",
    "parts = submission_merged['id1'].str.split(' ',n=1,expand=True)\n",
    "tp_sub = pd.to_datetime(parts[1], format='%H:%M:%S.%f', errors='coerce')\n",
    "submission_merged['event_seconds'] = (\n",
    "    tp_sub.dt.hour * 3600 +\n",
    "    tp_sub.dt.minute * 60 +\n",
    "    tp_sub.dt.second +\n",
    "    tp_sub.dt.microsecond / 1e-6\n",
    ")\n",
    "# *** New: Impression‐sequence per user for submission_merged\n",
    "submission_merged = submission_merged.sort_values(['id2','id4'])\n",
    "submission_merged['imp_seq'] = submission_merged.groupby('id2').cumcount() + 1\n",
    "submission_merged = enrich(submission_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67fb6be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Step 8: Target encoding for id6, id10, id11, f374, id8...\n"
     ]
    }
   ],
   "source": [
    "# ────────── Step 8: Target‐encoding masked & industry ──────────\n",
    "print(\"🔹 Step 8: Target encoding for id6, id10, id11, f374, id8...\")\n",
    "for col in ['id6','id10','id11','f374','id8']:\n",
    "    means      = train.groupby(col)['y'].mean()\n",
    "    global_mean= train['y'].mean()\n",
    "    train[f'{col}_te']             = train[col].map(means).fillna(global_mean)\n",
    "    test_local[f'{col}_te']        = test_local[col].map(means).fillna(global_mean)\n",
    "    submission_merged[f'{col}_te'] = submission_merged[col].map(means).fillna(global_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "811b0828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Step 9: Assembling feature list...\n"
     ]
    }
   ],
   "source": [
    "# ────── Step 9: Feature selection ──────\n",
    "print(\"🔹 Step 9: Assembling feature list...\")\n",
    "drop_cols = [\n",
    "    'id1','id2','id3','id4','id5','y',\n",
    "    'id6','id10','id11','id12','id13','last_tx',\n",
    "    'offer_impressions','offer_clicks'\n",
    "]\n",
    "features = [c for c in train.columns if c not in drop_cols and is_numeric_dtype(train[c])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33a1aaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Step 10: Creating train/val split by id2...\n"
     ]
    }
   ],
   "source": [
    "# ────────── Step 10: Group‐aware train/val split ──────────\n",
    "print(\"🔹 Step 10: Creating train/val split by id2...\")\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(train, groups=train['id2']))\n",
    "train_grp, val_grp = train.iloc[train_idx], train.iloc[val_idx]\n",
    "\n",
    "X_tr, y_tr, grp_tr = (\n",
    "    train_grp[features].fillna(-999),\n",
    "    train_grp['y'],\n",
    "    train_grp['id2']\n",
    ")\n",
    "X_val, y_val, grp_val = (\n",
    "    val_grp[features].fillna(-999),\n",
    "    val_grp['y'],\n",
    "    val_grp['id2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fcdbe680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Step 11: Computing group sizes & sorting...\n"
     ]
    }
   ],
   "source": [
    "# ────────── Step 11: Compute group sizes & sort ──────────\n",
    "print(\"🔹 Step 11: Computing group sizes & sorting...\")\n",
    "group_tr = train_grp.groupby('id2').size().values\n",
    "group_val= val_grp.groupby('id2').size().values\n",
    "\n",
    "X_tr  = X_tr .assign(id2=grp_tr) .sort_values('id2') .drop(columns='id2')\n",
    "y_tr  = y_tr .loc[X_tr.index]\n",
    "X_val = X_val.assign(id2=grp_val).sort_values('id2') .drop(columns='id2')\n",
    "y_val = y_val.loc[X_val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1600d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Step 12: Training LightGBMRanker...\n",
      "[LightGBM] [Info] Total groups: 37240, total data: 616615\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.112859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 49315\n",
      "[LightGBM] [Info] Number of data points in the train set: 616615, number of used features: 334\n",
      "[LightGBM] [Info] Total groups: 9310, total data: 153549\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[166]\tvalid_0's map@7: 0.943396\n",
      "✅ Model trained!\n"
     ]
    }
   ],
   "source": [
    "# ────────── Step 12: Train LGBMRanker ──────────\n",
    "print(\"🔹 Step 12: Training LightGBMRanker...\")\n",
    "ranker = lgb.LGBMRanker(\n",
    "    objective='lambdarank',\n",
    "    metric='map',\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=500,\n",
    "    num_leaves=31,\n",
    "    random_state=42\n",
    ")\n",
    "ranker.fit(\n",
    "    X_tr, y_tr,\n",
    "    group=group_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_group=[group_val],\n",
    "    eval_at=[7],\n",
    "    callbacks=[lgb.early_stopping(20)]\n",
    ")\n",
    "print(\"✅ Model trained!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b68bfbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Step 13: Predicting & normalizing local test...\n"
     ]
    }
   ],
   "source": [
    "# ────────── Step 13: Predict & normalize on local test ──────────\n",
    "print(\"🔹 Step 13: Predicting & normalizing local test...\")\n",
    "test_local['pred'] = ranker.predict(test_local[features].fillna(-999))\n",
    "test_local['pred'] = test_local.groupby('id2')['pred'] \\\n",
    "    .transform(lambda x: (x - x.min())/(x.max()-x.min()+1e-6))\n",
    "test_local[['id1','id2','id3','id5','pred']].to_csv(\n",
    "    'local_test_predictions_ranker.csv', index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "988d86c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Step 14: Predicting & normalizing submission...\n",
      "✅ All done! Submission file ready.\n"
     ]
    }
   ],
   "source": [
    "# ────────── Step 14: Predict & normalize for submission ──────────\n",
    "print(\"🔹 Step 14: Predicting & normalizing submission...\")\n",
    "submission_merged['pred'] = ranker.predict(submission_merged[features].fillna(-999))\n",
    "submission_merged['pred'] = submission_merged.groupby('id2')['pred'] \\\n",
    "    .transform(lambda x: (x - x.min())/(x.max()-x.min()+1e-6))\n",
    "\n",
    "submission_template = (\n",
    "    submission_template.drop(columns='pred', errors='ignore')\n",
    "    .merge(submission_merged[['id1','pred']], on='id1', how='left')\n",
    ")\n",
    "submission_template.to_csv(\n",
    "    'r2_submission_fileTeamPhoenix.csv', index=False\n",
    ")\n",
    "print(\"✅ All done! Submission file ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724759f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
